{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "print(device)\n",
    "\n",
    "random.seed(6) #For Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Network & Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,state_len,action_len):\n",
    "        super(DQN,self).__init__()\n",
    "        self.layer1 = nn.Linear(state_len, 16)\n",
    "        self.layer2 = nn.Linear(16, 16)\n",
    "        self.layer3 = nn.Linear(16, action_len)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Transition(1,2,3,4)\n",
    "#print(x.state) -> outputs 1\n",
    "#print(x.reward) -> outputs 4\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self,*args): #Converts State,Action,Next_State,Reward into transition tuple\n",
    "        self.memory.append(Transition(*args))  \n",
    "\n",
    "    def __sample__(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self): #len(Object of Replay Memory) -> internally calls __len__\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Policy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_net,state,steps):\n",
    "    global EPS_START ,EPS_END , EPS_DECAY\n",
    "    eps = EPS_START + (EPS_END - EPS_START)*math.exp(-1*(steps/EPS_DECAY))\n",
    "\n",
    "    if random.random() < eps : #Exploration\n",
    "        return random.randrange(2)  #Two Actions [\"JUMP or FALL\"]\n",
    "    else : #Exploitation \n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(state)).item() #Greedy Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net,target_net,memory,optimizer):\n",
    "    global BATCH_SIZE,GAMMA,device\n",
    "\n",
    "    if len(memory) < BATCH_SIZE : \n",
    "        return \n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE) #List of Transitions i.e (State Action Next_state Reward)\n",
    "    batch = Transition(*zip(*transitions)) #Transition of Lists i.e Transition[State = (s1,s2,..) Action = (a1,a2,..) ... Reward = (r1,r2,..)]\n",
    "    \n",
    "    state_batch  = torch.cat(batch.state)  #batch_size * no. of states  [from tuple of tensors to tensor]\n",
    "    action_batch = torch.cat(batch.action) #batch_size * no. of actions [from tuple of tensors to tensor]\n",
    "    reward_batch = torch.cat(batch.reward) #batch_size * no. of rewards [form tuple of tensors to tensor]\n",
    "\n",
    "    # [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] from neural network \n",
    "    #s1,s2,s3 -> state from memory buffer\n",
    "    #action1,action2,... -> action space\n",
    "    #a1,a2,...... -> action done when on s1,s2,...\n",
    "    #final result would be what is the quality of actions a1,a2,.. on states s1,s2,.. respectively\n",
    "    #i.e [Q(s1,action == a1),Q(s2,action == a2),Q(s3,action == a3),...]\n",
    "    state_action_values = policy_net(state_batch).gather(1,action_batch)\n",
    "\n",
    "    #Finding Expected State Action Values From Target Network\n",
    "    #next_state can be None sometimes, because next_state could be end of episodes\n",
    "    #So we ignore them\n",
    "    non_final_mask   = torch.tensor(tuple(map(lambda s : s is not None,batch.next_state)),device=device,dtype=torch.bool) \n",
    "    non_final_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    #Predicted Final Reward\n",
    "    #End of Episode = contributes 0\n",
    "    #But others have some value -> we get from target_net\n",
    "    #We know we get [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] out of neural network\n",
    "    #.max(1) -> ([Q(s1,action_i) max value, it's coordinates],[Q(s2,action_i) max value, it's coordinates],.....)\n",
    "    #.value  -> [Q(s1,action_i) max value, Q(s2,action_i) max value,...]\n",
    "    #essentially just fetching max value of the state\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device) \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_states).max(1).values\n",
    "    \n",
    "    #TD Estimate\n",
    "    expected_state_action_values = (GAMMA*next_state_values) + reward_batch \n",
    "    \n",
    "    #Standard Backpropagation\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.99 #Starting : Give Priority to Exploration\n",
    "EPS_END   = 0.01 #Ending   : Give Priority to Exploitation\n",
    "EPS_DECAY = 1000 #How Fast You Want to End Exploration\n",
    "\n",
    "BATCH_SIZE = 128 #SHOULD BE OBVIOUS XD\n",
    "GAMMA = 0.99 #Higher Gamma : Long Term Reward Maximization\n",
    "NUM_EPS = 50 #Each Episode[Level] We Make the Game Tougher\n",
    "TAU = 0.005  #Soft Update Factor For Updating targert_net towards policy_net\n",
    "LR = 1e-4    #Learning Rate for Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_len  = 5 #Pipe Coords - Player Coords,Space Between Pipes, Player Vertical Speed, Player Horizontal Speed\n",
    "action_len = 2 #JUMP or FALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_len, action_len).to(device)\n",
    "target_net = DQN(state_len, action_len).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) #Target Net is a Copy of Policy Net (Just few updates behind)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intially Params For Simulation\n",
    "#Params Get Harder Overtime\n",
    "#xGap - Decreases [Spacing B/w Pipes]\n",
    "#yGap - Increases [Spacing B/w Holes]\n",
    "#hGap - Decreases [Hole Space]\n",
    "#xSpeed - Player x movement Speed\n",
    "#ySpeed - Player y movement Speed\n",
    "env_config = { \n",
    "    \"xGap\": 400, #100\n",
    "    \"yGap\": 30, \n",
    "    \"hGap\": 200, #60\n",
    "    \"xSpeed\": 5, #10\n",
    "    \"ySpeed\": 4,\n",
    "}\n",
    "\n",
    "env = Proxy(level=env_config)\n",
    "action_map = [\"JUMP\",\"FALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    state_info = env.update(\"FALL\") #Dummy Action To Extract State Info\n",
    "    const_info = [env_config[\"ySpeed\"],env_config[\"xSpeed\"],env_config[\"hGap\"]] #Only Change Across Eps\n",
    "    state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32) \n",
    "    \n",
    "    survial_score = 0 #Continue With EPS Till 100 pillars are Crossed\n",
    "    survival_time = 0 #No. of steps Taken Before Reset\n",
    "    actions_done  = 0 #Counting the Number of Actions Done, Using which We Decay EPS\n",
    "    while survial_score < 100 :\n",
    "        #Loops Params\n",
    "        actions_done  += 1\n",
    "        survival_time += 1\n",
    "\n",
    "        #Perform Action & Observe\n",
    "        action = choose_action(policy_net,state,actions_done)\n",
    "        state_info = env.update(action_map[action])\n",
    "\n",
    "        done  = state_info['over']\n",
    "        score = state_info['score']\n",
    "\n",
    "        survial_score += score\n",
    "\n",
    "        if done :\n",
    "            survival_time = 0\n",
    "            survial_score = 0\n",
    "\n",
    "        #Next State\n",
    "        next_state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32)\n",
    "        reward = torch.tensor([survival_time*0.1 + score] ,device=device,dtype=torch.float32) #Pillar Crossed = +1 & Survive = +0.1\n",
    "\n",
    "        #Store in Memory\n",
    "        memory.push(state,action,next_state if not done else None,reward)\n",
    "\n",
    "        #Move to Next State\n",
    "        state = next_state\n",
    "\n",
    "        #Soft Update\n",
    "        #Target_Net_Wts = (1-TAU)*Target_Net_Wts + TAU*POLICY_Net_Wts\n",
    "        target_dict = target_net.state_dict()\n",
    "        policy_dict = policy_net.state_dict()\n",
    "        \n",
    "        for key in policy_dict:\n",
    "            target_dict[key] = policy_dict[key]*TAU + target_dict[key]*(1-TAU) \n",
    "            target_net.load_state_dict(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

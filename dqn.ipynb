{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Standard Python Imports\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#Custom Imports\n",
    "from proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e4ff6a4550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "print(device)\n",
    "\n",
    "random.seed(6) #For Consistency\n",
    "torch.manual_seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Network & Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,state_len,action_len):\n",
    "        super(DQN,self).__init__()\n",
    "        self.layer1 = nn.Linear(state_len, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64,64)\n",
    "        self.layer4 = nn.Linear(64, action_len)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.layer4(x) #No activation Function Here ^ ^\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Transition(1,2,3,4)\n",
    "#print(x.state) -> outputs 1\n",
    "#print(x.reward) -> outputs 4\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self,*args): #Converts State,Action,Next_State,Reward into transition tuple\n",
    "        self.memory.append(Transition(*args))  \n",
    "\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self): #len(Object of Replay Memory) -> internally calls __len__\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Policy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_net,state,steps):\n",
    "    global EPS_START ,EPS_END , EPS_DECAY\n",
    "    eps = EPS_END + (EPS_START - EPS_END)*math.exp(-1*(steps/EPS_DECAY))\n",
    "\n",
    "    if random.random() < eps : #Exploration\n",
    "        return \"random action\",random.randrange(2)  #Two Actions [\"JUMP or FALL\"]\n",
    "    else : #Exploitation \n",
    "        with torch.no_grad():\n",
    "            return \"planned action\",torch.argmax(policy_net(state)).item() #Greedy Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net,target_net,memory,optimizer):\n",
    "    global BATCH_SIZE,GAMMA,device\n",
    "\n",
    "    if len(memory) < BATCH_SIZE : \n",
    "        return \n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE) #List of Transitions i.e (State Action Next_state Reward)\n",
    "    batch = Transition(*zip(*transitions)) #Transition of Lists i.e Transition[State = (s1,s2,..) Action = (a1,a2,..) ... Reward = (r1,r2,..)]\n",
    "    \n",
    "    state_batch  = torch.cat(batch.state).reshape(BATCH_SIZE,-1)  #batch_size * no. of states  [from tuple of tensors to tensor]\n",
    "    action_batch = torch.cat(batch.action).reshape(BATCH_SIZE,-1) #batch_size * no. of actions [from tuple of tensors to tensor]\n",
    "    reward_batch = torch.cat(batch.reward) #batch_size * no. of rewards [form tuple of tensors to tensor]\n",
    "\n",
    "\n",
    "    # [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] from neural network \n",
    "    #s1,s2,s3 -> state from memory buffer\n",
    "    #action1,action2,... -> action space\n",
    "    #a1,a2,...... -> action done when on s1,s2,...\n",
    "    #final result would be what is the quality of actions a1,a2,.. on states s1,s2,.. respectively\n",
    "    #i.e [Q(s1,action == a1),Q(s2,action == a2),Q(s3,action == a3),...]\n",
    "    state_action_values = policy_net(state_batch).gather(1,action_batch)\n",
    "\n",
    "    #Finding Expected State Action Values From Target Network\n",
    "    #next_state can be None sometimes, because next_state could be end of episodes\n",
    "    #So we ignore them\n",
    "    non_final_mask   = torch.tensor(tuple(map(lambda s : s is not None,batch.next_state)),device=device,dtype=torch.bool) \n",
    "    non_final_states = torch.cat([s for s in batch.next_state if s is not None]).reshape(-1, state_batch.size(1))\n",
    "    \n",
    "    #Predicted Final Reward\n",
    "    #End of Episode = contributes 0\n",
    "    #But others have some value -> we get from target_net\n",
    "    #We know we get [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] out of neural network\n",
    "    #.max(1) -> ([Q(s1,action_i) max value, it's coordinates],[Q(s2,action_i) max value, it's coordinates],.....)\n",
    "    #.value  -> [Q(s1,action_i) max value, Q(s2,action_i) max value,...]\n",
    "    #essentially just fetching max value of the state\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device) \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_states).max(1).values\n",
    "    \n",
    "    #TD Estimate\n",
    "    expected_state_action_values = (GAMMA*next_state_values) + reward_batch \n",
    "\n",
    "    #Standard Backpropagation\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.reshape(BATCH_SIZE), expected_state_action_values.reshape(BATCH_SIZE))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.99  #Starting : Give Priority to Exploration\n",
    "EPS_END   = 0.01  #Ending   : Give Priority to Exploitation\n",
    "EPS_DECAY = 1000  #How Fast You Want to End Exploration\n",
    "\n",
    "BATCH_SIZE = 128 #SHOULD BE OBVIOUS XD\n",
    "GAMMA = 0.99 #Higher Gamma : Long Term Reward Maximization\n",
    "NUM_EPS = 10 #Each Episode[Level] We Make the Game Tougher\n",
    "TAU = 0.005  #Soft Update Factor For Updating targert_net towards policy_net\n",
    "LR = 1e-3    #Learning Rate for Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_len  = 5 #Pipe Coords - Player Coords,Space Between Pipes, Player Vertical Speed, Player Horizontal Speed\n",
    "action_len = 2 #JUMP or FALL\n",
    "action_map = [\"JUMP\",\"FALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_len, action_len).to(device)\n",
    "target_net = DQN(state_len, action_len).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) #Target Net is a Copy of Policy Net (Just few updates behind)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intial Params For Simulation\n",
    "#Params Get Harder Overtime\n",
    "#xGap - Decreases [Spacing B/w Pipes]\n",
    "#yGap - Increases [Spacing B/w Holes]\n",
    "#hGap - Decreases [Hole Space]\n",
    "#xSpeed - Player x movement Speed \n",
    "#ySpeed - Player y movement Speed Decreases\n",
    "env_config = { \n",
    "    \"xGap\": 200, #At Max Diff 100\n",
    "    \"yGap\": 30,  #At Max Diff 60\n",
    "    \"hGap\": 250, #At Max Diff 50\n",
    "    \"xSpeed\": 10, \n",
    "    \"ySpeed\": 10, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Proxy(level=env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Params For Simulation\n",
    "env_config_final = {\n",
    "    \"xGap\" : 100,\n",
    "    \"yGap\" : 90,\n",
    "    \"hGap\" : 80,\n",
    "    \"xSpeed\": 12,\n",
    "    \"ySpeed\": 10,\n",
    "}\n",
    "\n",
    "#Step Size of decrementing Each of the Params\n",
    "env_config_step_size = {\n",
    "    key : (env_config_final[key] - env_config[key])/NUM_EPS for key in env_config.keys()   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                     \r"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPS):\n",
    "    state_info = env.update(\"FALL\") #Dummy Action To Extract State Info\n",
    "    const_info = [env_config[\"ySpeed\"],env_config[\"xSpeed\"],env_config[\"hGap\"]] #Only Change Across Eps\n",
    "    state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32) \n",
    "    \n",
    "    survial_score = 0 #Continue With EPS Till 20 pillars are Crossed\n",
    "    survival_time = 0 #No. of steps Taken Before Reset\n",
    "    actions_done  = 0 #Counting the Number of Actions Done, Using which We Decay EPS\n",
    "    \n",
    "    with tqdm(total=50, desc=f\"Episode-{i} Survival Progress\", leave=False) as pbar: #Loading Bar\n",
    "        max_survival_score = 0\n",
    "        while survial_score < 50 :\n",
    "            #Loops Params\n",
    "            actions_done  += 1\n",
    "            survival_time += 1\n",
    "\n",
    "            #Perform Action & Observe\n",
    "            # Decays Faster as Toughness increases\n",
    "            actionType,action = choose_action(policy_net,state,actions_done*math.ceil( (i+1)/2) ) \n",
    "            state_info = env.update(action_map[action])\n",
    "\n",
    "            done  = state_info['over']\n",
    "            score = state_info['score']\n",
    "\n",
    "            survial_score += score\n",
    "\n",
    "            if done :\n",
    "                survival_time = 0\n",
    "                survial_score = 0\n",
    "\n",
    "            #Next State\n",
    "            #Reward : Pillar Crossed = +1 & die = -200\n",
    "            next_state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32)\n",
    "            reward = torch.tensor([score*10 - (20 if done else 0)] ,device=device,dtype=torch.float32) \n",
    "            action = torch.tensor([action],device=device)\n",
    "\n",
    "            #Store in Memory\n",
    "            memory.push(state,action,next_state if not done else None,reward)\n",
    "\n",
    "            #Move to Next State\n",
    "            state = next_state\n",
    "\n",
    "            #Optimize Model\n",
    "            optimize_model(policy_net,target_net,memory,optimizer)\n",
    "\n",
    "            #Soft Update\n",
    "            #Target_Net_Wts = (1-TAU)*Target_Net_Wts + TAU*POLICY_Net_Wts\n",
    "            target_dict = target_net.state_dict()\n",
    "            policy_dict = policy_net.state_dict()\n",
    "            \n",
    "            for key in policy_dict:\n",
    "                target_dict[key] = policy_dict[key]*TAU + target_dict[key]*(1-TAU) \n",
    "                target_net.load_state_dict(target_dict)\n",
    "\n",
    "            #Printing Progress\n",
    "            pbar.update(score)\n",
    "            max_survival_score = max(max_survival_score,survial_score)\n",
    "            pbar.set_postfix_str(f\"Steps: {survival_time}, Max Score: {max_survival_score}, Action Type: {actionType}\")\n",
    "\n",
    "            if done :\n",
    "                pbar.reset()\n",
    "\n",
    "    #Moving onto next level\n",
    "    #Updating Params Based on Step Size\n",
    "    new_config = {key : math.ceil(env_config[key] + i*env_config_step_size[key]) for key in env_config.keys()}\n",
    "    env.levelUp(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping Training Env\n",
    "env.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(target_net, policy_net, models_dir='models'):\n",
    "    # Ensure the models directory exists\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    # Get a list of all folder names in the models directory\n",
    "    existing_folders = [name for name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, name))]\n",
    "\n",
    "    # Find the largest folder name\n",
    "    if existing_folders:\n",
    "        largest_folder = max(int(folder) for folder in existing_folders)\n",
    "        new_folder = str(largest_folder + 1)\n",
    "    else:\n",
    "        new_folder = '0'\n",
    "\n",
    "    # Create the new folder\n",
    "    new_folder_path = os.path.join(models_dir, new_folder)\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "    # Save the target_net and policy_net in the new folder\n",
    "    torch.save(target_net.state_dict(), os.path.join(new_folder_path, 'target_net.pth'))\n",
    "    torch.save(policy_net.state_dict(), os.path.join(new_folder_path, 'policy_net.pth'))\n",
    "\n",
    "    print(f\"Models saved in folder: {new_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved in folder: models\\0\n"
     ]
    }
   ],
   "source": [
    "save_models(target_net, policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(target_net, policy_net, folder_name, models_dir='models'):\n",
    "    # Construct the path to the specific folder inside models\n",
    "    folder_path = os.path.join(models_dir, folder_name)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder {folder_name} not found in {models_dir}\")\n",
    "\n",
    "    # Load the target_net and policy_net\n",
    "    target_net_path = os.path.join(folder_path, 'target_net.pth')\n",
    "    target_net.load_state_dict(torch.load(target_net_path))\n",
    "    print(f\"Target_Net loaded from {folder_path}\")\n",
    "    \n",
    "    policy_net_path = os.path.join(folder_path, 'policy_net.pth')\n",
    "    policy_net.load_state_dict(torch.load(policy_net_path))\n",
    "    print(f\"Policy_Net loaded from {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target_Net loaded from models\\0\n",
      "Policy_Net loaded from models\\0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laksh\\AppData\\Local\\Temp\\ipykernel_15880\\305100144.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target_net.load_state_dict(torch.load(target_net_path))\n",
      "C:\\Users\\laksh\\AppData\\Local\\Temp\\ipykernel_15880\\305100144.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(policy_net_path))\n"
     ]
    }
   ],
   "source": [
    "folder_name = '0' \n",
    "load_models(target_net, policy_net, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env_config = {\"xGap\": 120, \"yGap\": 78, \"hGap\": 114, \"xSpeed\": 12, \"ySpeed\": 10}\n",
    "test_env  = Proxy(level=test_env_config)\n",
    "max_score = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    }
   ],
   "source": [
    "state_info = test_env.update(\"FALL\") #Dummy Action To Extract State Info\n",
    "const_info = [test_env_config[\"ySpeed\"],test_env_config[\"xSpeed\"],test_env_config[\"hGap\"]]\n",
    "state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32) \n",
    "\n",
    "survial_score = 0 \n",
    "survival_time = 0 \n",
    "\n",
    "with tqdm(total=max_score, desc=f\"Survival Progress\", leave=False) as pbar: #Loading Bar\n",
    "    while survial_score < max_score :\n",
    "        #Loops Params\n",
    "        survival_time += 1\n",
    "\n",
    "        #Perform Action & Observe\n",
    "        action = 0\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy_net(state)).item()\n",
    "        state_info = test_env.update(action_map[action])\n",
    "\n",
    "        #Observations\n",
    "        done  = state_info['over']\n",
    "        score = state_info['score']\n",
    "        survial_score += score\n",
    "        if done :\n",
    "            survival_time = 0\n",
    "            survial_score = 0\n",
    "\n",
    "        #Next State\n",
    "        next_state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32)\n",
    "        state = next_state\n",
    "\n",
    "        #Printing Progress\n",
    "        pbar.update(score)\n",
    "        pbar.set_postfix_str(f\"Steps: {survival_time}\")\n",
    "\n",
    "        if done :\n",
    "            pbar.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

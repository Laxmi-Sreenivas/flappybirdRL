{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
    "print(device)\n",
    "\n",
    "random.seed(6) #For Consistency\n",
    "torch.manual_seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Network & Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,state_len,action_len):\n",
    "        super(DQN,self).__init__()\n",
    "        self.layer1 = nn.Linear(state_len, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64,64)\n",
    "        self.layer4 = nn.Linear(64, action_len)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Transition(1,2,3,4)\n",
    "#print(x.state) -> outputs 1\n",
    "#print(x.reward) -> outputs 4\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self,*args): #Converts State,Action,Next_State,Reward into transition tuple\n",
    "        self.memory.append(Transition(*args))  \n",
    "\n",
    "    def __sample__(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "\n",
    "    def __len__(self): #len(Object of Replay Memory) -> internally calls __len__\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon Greedy Policy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(policy_net,state,steps):\n",
    "    global EPS_START ,EPS_END , EPS_DECAY\n",
    "    eps = EPS_START + (EPS_END - EPS_START)*math.exp(-1*(steps/EPS_DECAY))\n",
    "\n",
    "    if random.random() < eps : #Exploration\n",
    "        return random.randrange(2)  #Two Actions [\"JUMP or FALL\"]\n",
    "    else : #Exploitation \n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(state)).item() #Greedy Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net,target_net,memory,optimizer):\n",
    "    global BATCH_SIZE,GAMMA,device\n",
    "\n",
    "    if len(memory) < BATCH_SIZE : \n",
    "        return \n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE) #List of Transitions i.e (State Action Next_state Reward)\n",
    "    batch = Transition(*zip(*transitions)) #Transition of Lists i.e Transition[State = (s1,s2,..) Action = (a1,a2,..) ... Reward = (r1,r2,..)]\n",
    "    \n",
    "    state_batch  = torch.cat(batch.state)  #batch_size * no. of states  [from tuple of tensors to tensor]\n",
    "    action_batch = torch.cat(batch.action) #batch_size * no. of actions [from tuple of tensors to tensor]\n",
    "    reward_batch = torch.cat(batch.reward) #batch_size * no. of rewards [form tuple of tensors to tensor]\n",
    "\n",
    "    # [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] from neural network \n",
    "    #s1,s2,s3 -> state from memory buffer\n",
    "    #action1,action2,... -> action space\n",
    "    #a1,a2,...... -> action done when on s1,s2,...\n",
    "    #final result would be what is the quality of actions a1,a2,.. on states s1,s2,.. respectively\n",
    "    #i.e [Q(s1,action == a1),Q(s2,action == a2),Q(s3,action == a3),...]\n",
    "    state_action_values = policy_net(state_batch).gather(1,action_batch)\n",
    "\n",
    "    #Finding Expected State Action Values From Target Network\n",
    "    #next_state can be None sometimes, because next_state could be end of episodes\n",
    "    #So we ignore them\n",
    "    non_final_mask   = torch.tensor(tuple(map(lambda s : s is not None,batch.next_state)),device=device,dtype=torch.bool) \n",
    "    non_final_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    #Predicted Final Reward\n",
    "    #End of Episode = contributes 0\n",
    "    #But others have some value -> we get from target_net\n",
    "    #We know we get [[Q(s1,action1),Q(s1,action2),...],[Q(s2,action1),Q(s2,action2),...],...] out of neural network\n",
    "    #.max(1) -> ([Q(s1,action_i) max value, it's coordinates],[Q(s2,action_i) max value, it's coordinates],.....)\n",
    "    #.value  -> [Q(s1,action_i) max value, Q(s2,action_i) max value,...]\n",
    "    #essentially just fetching max value of the state\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device) \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_states).max(1).values\n",
    "    \n",
    "    #TD Estimate\n",
    "    expected_state_action_values = (GAMMA*next_state_values) + reward_batch \n",
    "    \n",
    "    #Standard Backpropagation\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.99  #Starting : Give Priority to Exploration\n",
    "EPS_END   = 0.01  #Ending   : Give Priority to Exploitation\n",
    "EPS_DECAY = 100   #How Fast You Want to End Exploration\n",
    "\n",
    "BATCH_SIZE = 128 #SHOULD BE OBVIOUS XD\n",
    "GAMMA = 0.99 #Higher Gamma : Long Term Reward Maximization\n",
    "NUM_EPS = 50 #Each Episode[Level] We Make the Game Tougher\n",
    "TAU = 0.005  #Soft Update Factor For Updating targert_net towards policy_net\n",
    "LR = 1e-3    #Learning Rate for Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_len  = 5 #Pipe Coords - Player Coords,Space Between Pipes, Player Vertical Speed, Player Horizontal Speed\n",
    "action_len = 2 #JUMP or FALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_len, action_len).to(device)\n",
    "target_net = DQN(state_len, action_len).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict()) #Target Net is a Copy of Policy Net (Just few updates behind)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intial Params For Simulation\n",
    "#Params Get Harder Overtime\n",
    "#xGap - Decreases [Spacing B/w Pipes]\n",
    "#yGap - Increases [Spacing B/w Holes]\n",
    "#hGap - Decreases [Hole Space]\n",
    "#xSpeed - Player x movement Speed \n",
    "#ySpeed - Player y movement Speed Decreases\n",
    "env_config = { \n",
    "    \"xGap\": 200, #At Max Diff 100\n",
    "    \"yGap\": 30,  #At Max Diff 60\n",
    "    \"hGap\": 250, #At Max Diff 50\n",
    "    \"xSpeed\": 10, \n",
    "    \"ySpeed\": 4, #At Max Diff 1\n",
    "}\n",
    "\n",
    "env = Proxy(level=env_config)\n",
    "action_map = [\"JUMP\",\"FALL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Params For Simulation\n",
    "env_config_final = {\n",
    "    \"xGap\" : 50,\n",
    "    \"yGap\" : 60,\n",
    "    \"hGap\" : 100,\n",
    "    \"xSpeed\": 10,\n",
    "    \"ySpeed\": 4,\n",
    "}\n",
    "\n",
    "#Step Size of decrementing Each of the Params\n",
    "env_config_step_size = {\n",
    "    key : (env_config_final[key] - env_config[key])//NUM_EPS for key in env_config.keys()   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_EPS):\n",
    "    state_info = env.update(\"FALL\") #Dummy Action To Extract State Info\n",
    "    const_info = [env_config[\"ySpeed\"],env_config[\"xSpeed\"],env_config[\"hGap\"]] #Only Change Across Eps\n",
    "    state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32) \n",
    "    \n",
    "    survial_score = 0 #Continue With EPS Till 20 pillars are Crossed\n",
    "    survival_time = 0 #No. of steps Taken Before Reset\n",
    "    actions_done  = 0 #Counting the Number of Actions Done, Using which We Decay EPS\n",
    "    \n",
    "    with tqdm(total=10, desc=f\"Episode-{i} Survival Progress\", leave=False) as pbar: #Loading Bar\n",
    "        max_survival_score = 0\n",
    "        while survial_score < 10 :\n",
    "            #Loops Params\n",
    "            actions_done  += 1\n",
    "            survival_time += 1\n",
    "\n",
    "            #Perform Action & Observe\n",
    "            action = choose_action(policy_net,state,survival_time)\n",
    "            state_info = env.update(action_map[action])\n",
    "\n",
    "            done  = state_info['over']\n",
    "            score = state_info['score']\n",
    "\n",
    "            survial_score += score\n",
    "\n",
    "            if done :\n",
    "                survival_time = 0\n",
    "                survial_score = 0\n",
    "\n",
    "            #Next State\n",
    "            next_state = torch.tensor(state_info[\"pos\"] + const_info,device=device,dtype=torch.float32)\n",
    "            reward = torch.tensor([score*10 - (200 if done else 0)] ,device=device,dtype=torch.float32) #Pillar Crossed = +1 & Survive = +0.1\n",
    "\n",
    "            #Store in Memory\n",
    "            memory.push(state,action,next_state if not done else None,reward)\n",
    "\n",
    "            #Move to Next State\n",
    "            state = next_state\n",
    "\n",
    "            #Soft Update\n",
    "            #Target_Net_Wts = (1-TAU)*Target_Net_Wts + TAU*POLICY_Net_Wts\n",
    "            target_dict = target_net.state_dict()\n",
    "            policy_dict = policy_net.state_dict()\n",
    "            \n",
    "            for key in policy_dict:\n",
    "                target_dict[key] = policy_dict[key]*TAU + target_dict[key]*(1-TAU) \n",
    "                target_net.load_state_dict(target_dict)\n",
    "\n",
    "            #Printing Progress\n",
    "            pbar.update(score)\n",
    "            max_survival_score = max(max_survival_score,survial_score)\n",
    "            pbar.set_postfix_str(f\"Steps: {survival_time}, Max Score: {max_survival_score}\")\n",
    "\n",
    "            if done :\n",
    "                pbar.reset()\n",
    "\n",
    "    #Moving onto next level\n",
    "    #Updating Params Based on Step Size\n",
    "    env_config = {key : env_config[key] + env_config_step_size[key] for key in env_config.keys()}\n",
    "    env.levelUp(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping Training Env\n",
    "env.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(target_net, policy_net, models_dir='models'):\n",
    "    # Ensure the models directory exists\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "\n",
    "    # Get a list of all folder names in the models directory\n",
    "    existing_folders = [name for name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, name))]\n",
    "\n",
    "    # Find the largest folder name\n",
    "    if existing_folders:\n",
    "        largest_folder = max(int(folder) for folder in existing_folders)\n",
    "        new_folder = str(largest_folder + 1)\n",
    "    else:\n",
    "        new_folder = '0'\n",
    "\n",
    "    # Create the new folder\n",
    "    new_folder_path = os.path.join(models_dir, new_folder)\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "    # Save the target_net and policy_net in the new folder\n",
    "    torch.save(target_net.state_dict(), os.path.join(new_folder_path, 'target_net.pth'))\n",
    "    torch.save(policy_net.state_dict(), os.path.join(new_folder_path, 'policy_net.pth'))\n",
    "\n",
    "    print(f\"Models saved in folder: {new_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(target_net, policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(target_net, policy_net, folder_name, models_dir='models'):\n",
    "    # Construct the path to the specific folder inside models\n",
    "    folder_path = os.path.join(models_dir, folder_name)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder {folder_name} not found in {models_dir}\")\n",
    "\n",
    "    # Load the target_net and policy_net\n",
    "    target_net_path = os.path.join(folder_path, 'target_net.pth')\n",
    "    target_net.load_state_dict(torch.load(target_net_path))\n",
    "    print(f\"Target_Net loaded from {folder_path}\")\n",
    "    \n",
    "    policy_net_path = os.path.join(folder_path, 'policy_net.pth')\n",
    "    policy_net.load_state_dict(torch.load(policy_net_path))\n",
    "    print(f\"Policy_Net loaded from {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = '0' \n",
    "load_models(target_net, policy_net, folder_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
